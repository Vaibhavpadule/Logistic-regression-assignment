{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression | Assignment"
      ],
      "metadata": {
        "id": "E4W00mEcUfCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?**"
      ],
      "metadata": {
        "id": "Xf9tKzx3Ukdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression vs. Linear Regression**\n",
        "\n",
        "### ‚úÖ **Logistic Regression:**\n",
        "\n",
        "* **Purpose:** Used for **classification** problems, typically binary classification (e.g., yes/no, spam/not spam).\n",
        "\n",
        "* **Output:** Predicts the **probability** that a given input belongs to a particular class (value between **0 and 1**).\n",
        "\n",
        "* **Function Used:** Uses the **logistic (sigmoid)** function to squash the output of a linear equation into the range \\[0, 1].\n",
        "\n",
        "  $$\n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\text{ where } z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n\n",
        "  $$\n",
        "\n",
        "* **Decision Boundary:** A threshold (commonly 0.5) is applied to classify the result into one of the categories.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Linear Regression:**\n",
        "\n",
        "* **Purpose:** Used for **regression** problems, i.e., predicting continuous numerical values.\n",
        "\n",
        "* **Output:** Predicts an output that can range from **-‚àû to +‚àû**.\n",
        "\n",
        "* **Function Used:** Fits a **straight line** (or hyperplane in higher dimensions) to the data.\n",
        "\n",
        "  $$\n",
        "  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n\n",
        "  $$\n",
        "\n",
        "* **Loss Function:** Uses **Mean Squared Error (MSE)** to minimize prediction errors.\n",
        "\n",
        "---\n",
        "\n",
        "### üîë **Key Differences:**\n",
        "\n",
        "| Feature                 | Linear Regression      | Logistic Regression                |\n",
        "| ----------------------- | ---------------------- | ---------------------------------- |\n",
        "| **Type of Problem**     | Regression             | Classification                     |\n",
        "| **Output**              | Continuous value       | Probability (0 to 1)               |\n",
        "| **Prediction Function** | Linear equation        | Logistic (sigmoid) function        |\n",
        "| **Loss Function**       | Mean Squared Error     | Log Loss (Cross-Entropy)           |\n",
        "| **Use Case Example**    | Predicting house price | Predicting if email is spam or not |\n",
        "\n"
      ],
      "metadata": {
        "id": "dm92gj-IUpGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the role of the Sigmoid function in Logistic Regression.**"
      ],
      "metadata": {
        "id": "EeqjoQ0sU3WK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ **Role of the Sigmoid Function in Logistic Regression**\n",
        "\n",
        "The **sigmoid function** is at the heart of logistic regression. Its primary role is to **map any real-valued number (from $-\\infty$ to $+\\infty$) into a value between 0 and 1**, which can then be interpreted as a **probability**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Definition of the Sigmoid Function:**\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $z$ is the output of the linear combination of features:\n",
        "\n",
        "  $$\n",
        "  z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Role in Logistic Regression:**\n",
        "\n",
        "1. **Transforms Linear Output to Probability:**\n",
        "\n",
        "   * Logistic regression first computes a **linear score** using the input features and weights.\n",
        "   * The **sigmoid function squashes** this linear score into a range between 0 and 1.\n",
        "   * This makes it suitable for **binary classification**, as it gives the probability of the input belonging to the **positive class** (usually labeled as 1).\n",
        "\n",
        "2. **Helps with Classification:**\n",
        "\n",
        "   * The model uses a **threshold** (commonly 0.5) on the output of the sigmoid function to make a **classification decision**:\n",
        "\n",
        "     $$\n",
        "     \\text{If } \\sigma(z) \\geq 0.5 \\Rightarrow \\text{class 1 (positive)}\n",
        "     $$\n",
        "\n",
        "     $$\n",
        "     \\text{If } \\sigma(z) < 0.5 \\Rightarrow \\text{class 0 (negative)}\n",
        "     $$\n",
        "\n",
        "3. **Probabilistic Interpretation:**\n",
        "\n",
        "   * The sigmoid function enables logistic regression to output **probabilities**, which is useful not just for classification, but also for **confidence estimation** and **ranking**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà **Graphical Behavior:**\n",
        "\n",
        "* The sigmoid function has an \"S\"-shaped curve.\n",
        "* As $z \\to +\\infty$, $\\sigma(z) \\to 1$\n",
        "* As $z \\to -\\infty$, $\\sigma(z) \\to 0$\n",
        "* At $z = 0$, $\\sigma(z) = 0.5$\n",
        "\n",
        "---\n",
        "\n",
        "### üß† In Summary:\n",
        "\n",
        "> The sigmoid function converts the linear output of the model into a **probability**, enabling logistic regression to perform **binary classification** in a smooth and differentiable way.\n"
      ],
      "metadata": {
        "id": "JbEiWTpVVDi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Regularization in Logistic Regression and why is it needed?**"
      ],
      "metadata": {
        "id": "Zf2l3NCSVF1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ **What is Regularization in Logistic Regression?**\n",
        "\n",
        "**Regularization** is a technique used in **logistic regression (and other models)** to **prevent overfitting** by discouraging overly complex models. It works by adding a **penalty term** to the loss function, which **controls the size of the model coefficients**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Why is Regularization Needed?**\n",
        "\n",
        "* In logistic regression, the model learns weights ($\\beta$) for each feature.\n",
        "* If the model **fits the training data too well**, it might learn noise or irrelevant patterns ‚Äî this is **overfitting**.\n",
        "* Overfitting leads to **poor generalization** to new (unseen) data.\n",
        "* Regularization helps by **penalizing large weights**, encouraging the model to keep them small and simpler.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ **How Does It Work?**\n",
        "\n",
        "#### Original Loss Function (Log Loss):\n",
        "\n",
        "$$\n",
        "J(\\beta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "> Where:\n",
        ">\n",
        "> * $m$ is the number of training examples\n",
        "> * $\\hat{y}^{(i)}$ is the predicted probability\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú≥Ô∏è **With Regularization:**\n",
        "\n",
        "There are two common types:\n",
        "\n",
        "#### 1. **L2 Regularization (Ridge):**\n",
        "\n",
        "Adds the **sum of squares of weights** to the loss:\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
        "$$\n",
        "\n",
        "* **Effect:** Penalizes large weights smoothly.\n",
        "* **Commonly used in logistic regression** by default in most libraries (like `sklearn`).\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **L1 Regularization (Lasso):**\n",
        "\n",
        "Adds the **sum of absolute values of weights**:\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
        "$$\n",
        "\n",
        "* **Effect:** Drives some weights to exactly **zero**, performing **feature selection**.\n",
        "* Useful when you have many irrelevant or noisy features.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è **What is Œª (lambda)?**\n",
        "\n",
        "* It's the **regularization parameter** (also sometimes called `C` in `sklearn`, where `C = 1/Œª`).\n",
        "* Controls the strength of the penalty:\n",
        "\n",
        "  * **Large Œª ‚Üí more regularization ‚Üí simpler model**\n",
        "  * **Small Œª ‚Üí less regularization ‚Üí more complex model**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **In Summary:**\n",
        "\n",
        "> **Regularization in logistic regression** helps control **model complexity** by adding a penalty for large coefficients. This **prevents overfitting**, improves **generalization**, and can even perform **feature selection** (in L1).\n"
      ],
      "metadata": {
        "id": "dta2TFYcVLWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are some common evaluation metrics for classification models, and\n",
        "why are they important?**"
      ],
      "metadata": {
        "id": "yaLA1VuBVX4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ **Common Evaluation Metrics for Classification Models**\n",
        "\n",
        "Evaluation metrics are essential to understand **how well a classification model performs**, especially in real-world applications where accuracy alone may not tell the full story.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **1. Accuracy**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
        "$$\n",
        "\n",
        "* **Good for:** Balanced datasets (equal number of classes).\n",
        "* **Limitation:** Misleading on **imbalanced datasets** (e.g., predicting all \"No\" in a 95% \"No\" dataset gives 95% accuracy but zero usefulness).\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **2. Precision**\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
        "$$\n",
        "\n",
        "* Measures: **How many of the predicted positives are actually correct**.\n",
        "* **High precision** = Low false positive rate.\n",
        "* **Useful when**: False positives are costly (e.g., spam filters, medical diagnosis).\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **3. Recall (Sensitivity / True Positive Rate)**\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
        "$$\n",
        "\n",
        "* Measures: **How many actual positives were correctly identified**.\n",
        "* **High recall** = Low false negative rate.\n",
        "* **Useful when**: Missing a positive is costly (e.g., cancer detection, fraud detection).\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **4. F1 Score**\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}\n",
        "$$\n",
        "\n",
        "* **Harmonic mean** of precision and recall.\n",
        "* Balances precision and recall when both are important.\n",
        "* **Useful when:** You need a balance between false positives and false negatives.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **5. Confusion Matrix**\n",
        "\n",
        "A **2x2 table** for binary classification:\n",
        "\n",
        "|                     | Predicted Positive  | Predicted Negative  |\n",
        "| ------------------- | ------------------- | ------------------- |\n",
        "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
        "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "* Gives a **complete breakdown** of classification results.\n",
        "* Basis for many other metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **6. ROC Curve & AUC (Area Under Curve)**\n",
        "\n",
        "* **ROC Curve:** Plots **True Positive Rate vs False Positive Rate** at various thresholds.\n",
        "\n",
        "* **AUC (Area Under Curve):** Measures the **overall ability** of the model to discriminate between classes.\n",
        "\n",
        "  * **AUC = 1:** Perfect classifier\n",
        "  * **AUC = 0.5:** Random guessing\n",
        "\n",
        "* **Useful when:** Evaluating performance across different classification thresholds.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **7. Log Loss (Cross-Entropy Loss)**\n",
        "\n",
        "* Measures the **uncertainty** of predictions.\n",
        "* Penalizes confident but **wrong predictions** more heavily.\n",
        "* Used especially when models output **probabilities**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Why Are These Metrics Important?**\n",
        "\n",
        "* **Different metrics reveal different strengths and weaknesses** of your model.\n",
        "* They help choose the **right model** for your specific use case.\n",
        "* In many applications, the **cost of false positives and false negatives differs** ‚Äî so **accuracy alone is not enough**.\n",
        "\n",
        "---\n",
        "\n",
        "### üö® Example:\n",
        "\n",
        "In a **disease detection model**:\n",
        "\n",
        "* Accuracy = 95% might sound great.\n",
        "* But if 95% of people are healthy, the model might just be predicting ‚Äúhealthy‚Äù all the time.\n",
        "* You need **recall** to ensure it catches sick patients and **precision** to avoid false alarms.\n"
      ],
      "metadata": {
        "id": "oA7dTkq0VcUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.**"
      ],
      "metadata": {
        "id": "ArmHFfkMVo00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 3. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # Increased max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABKIn09BV4MF",
        "outputId": "13849d0a-0dec-428d-a5a4-10b52a76ac88"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.**"
      ],
      "metadata": {
        "id": "mLEj1xHwWBzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Logistic Regression with L2 Regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print model coefficients and accuracy\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6cTgOU7WK9s",
        "outputId": "16119ed5-8b92-44b5-98ff-36eee4221a40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "mean radius: 1.0274\n",
            "mean texture: 0.2215\n",
            "mean perimeter: -0.3621\n",
            "mean area: 0.0255\n",
            "mean smoothness: -0.1562\n",
            "mean compactness: -0.2377\n",
            "mean concavity: -0.5326\n",
            "mean concave points: -0.2837\n",
            "mean symmetry: -0.2267\n",
            "mean fractal dimension: -0.0365\n",
            "radius error: -0.0971\n",
            "texture error: 1.3706\n",
            "perimeter error: -0.1814\n",
            "area error: -0.0872\n",
            "smoothness error: -0.0225\n",
            "compactness error: 0.0474\n",
            "concavity error: -0.0429\n",
            "concave points error: -0.0324\n",
            "symmetry error: -0.0347\n",
            "fractal dimension error: 0.0116\n",
            "worst radius: 0.1117\n",
            "worst texture: -0.5089\n",
            "worst perimeter: -0.0156\n",
            "worst area: -0.0169\n",
            "worst smoothness: -0.3077\n",
            "worst compactness: -0.7727\n",
            "worst concavity: -1.4286\n",
            "worst concave points: -0.5109\n",
            "worst symmetry: -0.7469\n",
            "worst fractal dimension: -0.1009\n",
            "\n",
            "Intercept: 28.6487\n",
            "\n",
            "Test Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.**"
      ],
      "metadata": {
        "id": "-BjsKeYqWURL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load a multiclass dataset (Iris)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Logistic Regression with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nELpjflVWbVC",
        "outputId": "c2271319-fcad-422f-884b-737a03aa2bfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.**"
      ],
      "metadata": {
        "id": "PACURU7TWn8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Set up parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],         # Regularization strength (smaller C = more regularization)\n",
        "    'penalty': ['l1', 'l2'],              # Regularization type\n",
        "    'solver': ['liblinear']               # 'liblinear' supports both l1 and l2\n",
        "}\n",
        "\n",
        "# 4. Initialize Logistic Regression and GridSearchCV\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# 5. Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EsNt0CYWt5D",
        "outputId": "e0e55cce-f148-40ac-c59a-dccd8c73a8c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Accuracy: 0.9670\n",
            "Test Set Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.**"
      ],
      "metadata": {
        "id": "ntivv1FBW6xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# Model WITHOUT Standardization\n",
        "# -----------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# Model WITH Standardization\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=1000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# -----------------------------\n",
        "# Compare and Print Results\n",
        "# -----------------------------\n",
        "print(f\"Accuracy WITHOUT Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH Scaling:    {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv_3QoYPXAbf",
        "outputId": "6a0727db-a9de-4274-c2f4-648a93701c8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.9561\n",
            "Accuracy WITH Scaling:    0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}